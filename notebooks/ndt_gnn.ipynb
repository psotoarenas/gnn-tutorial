{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3: Building an NDT for Next-Generation WLANs with Graph Neural Networks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook will process the provided dataset to create a NDT using techniques such as Graph Neural Networks (GNNs). The dataset was created for the 2020 edition of the [ITU AI for 5G Challenge](https://challenge.aiforgood.itu.int/) in the wireless throughput prediction statement. The dataset was created using [Komondor](https://github.com/wn-upf/Komondor), a simulator for next-generation, high-density WLANs which includes novel functionalities such as channel bonding and spatial reuse. The generated data includes simulated data from IEEE 802.11 WLAN deployments applying Dynamic Channel Bonding, using different random parameters, including channel allocation, location of nodes, and number of STAs per AP. The original dataset can be found in [here](https://doi.org/10.5281/zenodo.4106127). Then, the dataset was parsed using the file ```parse_original_dataset.py``` and then uploaded to Google Drive.  We include a fixed 80/20 split, where, from the original dataset (600 deployments in total), 80% of the deployments were used for training and while the remaining 20% was used for validation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To process the dataset for GNNs, we heavily rely on PyTorch Geometric, a library built upon PyTorch to easily write and train GNN models for a wide range of applications related to structured data.\n",
    "\n",
    "Packages such as jupyter, pandas, numpy and scikit-learn are already installed in a instance of Google Colab. Also PyTorch is already included, so there is no need to install them. However, you need to install the PyTorch Geometric version that satisfies the PyTorch version.\n",
    "\n",
    "Run first the following commands to find out which PyTorch and CUDA version you are using.\n",
    "```\n",
    "import torch\n",
    "\n",
    "def format_pytorch_version(version):\n",
    "  return version.split('+')[0]\n",
    "\n",
    "def format_cuda_version(version):\n",
    "  return 'cu' + version.replace('.', '')\n",
    "\n",
    "\n",
    "TORCH_version = torch.__version__\n",
    "print(f\"you are using pytorch and cuda {TORCH_version}\")\n",
    "TORCH = format_pytorch_version(TORCH_version)\n",
    "print(f\"you are using pytorch {TORCH}\")\n",
    "\n",
    "CUDA_version = torch.version.cuda\n",
    "print(f\"you are using cuda {CUDA_version}\")\n",
    "CUDA = format_cuda_version(CUDA_version)\n",
    "```\n",
    "\n",
    "Then you can run in a cell the corresponding pip command to install pytorch geometric.\n",
    "\n",
    "```\n",
    "! pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-{TORCH}+{CUDA}.html\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the notebook is run through Google Colab, then clone the repository and upload the root folder (e.g., gnn-tutorial) to your personal Drive. Then, mount your Drive, so you can import the dataset module using:\n",
    "\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "```\n",
    "Then, you can check the content mounted at ```/content/gdrive``` using:\n",
    "\n",
    "```\n",
    "! ls /content/gdrive/My\\ Drive/\n",
    "```\n",
    "We'll need to update our path to import from Drive using:\n",
    "\n",
    "```\n",
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/<path to dataset_gnn.py>')\n",
    "```\n",
    "\n",
    "Remember to replace the path where you can locate the ```dataset_gnn.py```. Now we can import the library and use the function with:\n",
    "\n",
    "```\n",
    "import dataset_gnn\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assuming that you meet all the requirements from above, we start by importing the necessary libraries. Remember to use ```import dataset_gnn``` instead of ```from auxiliaries import dataset_gnn``` and ```from evaluation import scores``` instead of ```from auxiliaries.evaluation import scores``` if this notebook is run in Google Colab."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Standard python libraries.\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "\n",
    "# General data science libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Deep learning libraries.\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MetaLayer\n",
    "from torch_scatter import scatter_mean\n",
    "\n",
    "# now we can import all the functions from the auxiliaries' folder, depending on where you are executing this notebook, you might use import dataset_gnn instead of the line below\n",
    "from auxiliaries import dataset_gnn\n",
    "# if executed in google colab, replace the line below with from evaluation import scores, access_point_throughputs\n",
    "from auxiliaries.evaluation import scores, access_point_throughputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we proceed to download, extract, and accommodate the dataset in a proper format to be used by the GNN model we create later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Dataset.\n",
    "########################################################################################################################\n",
    "\n",
    "root = './datasets/NDTDataset/gnn/'\n",
    "\n",
    "# Load training dataset.\n",
    "dataset_train = dataset_gnn.NDTDataset(root, split='train')\n",
    "dataset_valid = dataset_gnn.NDTDataset(root, split='valid')\n",
    "dataset_test = dataset_gnn.NDTDataset(root, split='test')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset is comprised from two kind of files, one is the \"input_node_files.csv\" which contains the deployment information and their respective configuration (e.g., channels bonded, node location, WLAN code). This information is given to Komondor which simulates 100 seconds of transmission and the output from the simulator is given in the \"output_simulator\" files (e.g., airtime, obtained throughput, interference and measured SINR and RSSI). We use content from both files to compose the graph's features. The features taken from the original dataset are the following.\n",
    "\n",
    "1. 'node_type': the considered node is an AP (0) or a STA (1). --> Node Feature\n",
    "2. 'node_x_coordinate': the x coordinate of the node spatial position. --> Node Feature\n",
    "3. 'node_y_coordinate': the y coordinate of the node spatial position. --> Node Feature\n",
    "4. 'node_primary_channel': the primary channel of the AP. --> Node Feature\n",
    "5. 'min_channel_allowed': the minimum channel allowed to perform channel bonding. --> Node Feature\n",
    "6. 'max_channel_allowed': the maximum channel allowed to perform channel bonding. --> Node Feature\n",
    "7. 'sinr': the obtained Signal plus Interference to Noise Ratio (SINR) between the AP and the STA. The feature is 0 for AP devices. --> Edge (AP-STA) Feature\n",
    "8. 'mean_airtime': the mean percentage of airtime given by the used channel. The feature is 0 for STAs.  --> Node Feature\n",
    "9. 'interference': the mean measured interferences between APs. --> Edge (AP-AP) Feature\n",
    "10. 'rssi': the obtained Received Signal Strength Indicator (RSSI) between the AP and the STA. --> Edge (AP-STA) Feature\n",
    "11. 'distance_to_ap': the euclidean distance between AP and STA. --> Edge (AP-STA) Feature\n",
    "12. 'bandwidth': the total bandwidth used. We consider 20 MHz per used channel. --> Edge (AP-STA) Feature\n",
    "\n",
    "There are two kind of nodes, APs and STAs, which we differentiate through the 'node_type' feature. Similarly, there are two kind of edges, an edge between APs (i.e., AP-AP edge) and an edge between AP and associated STAs (i.e., AP-STA edge). We assign features that are relevant for the AP-STA relationship (e.g., RSSI) but since this feature has no meaning in the AP-AP edges, the feature in those edges is set to zero. During its processing we encode the channel configuration ('node_primary_channel', 'min_channel_allowed', 'max_channel_allowed') using one hot encoder, which extends it into a vector of six positions. Let's load the dataset in memory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select the batch size.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Dataset loaders.\n",
    "train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE)         # we are loading 32 graphs each time\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=1)                  # we are validating one graph at a time\n",
    "test_loader = DataLoader(dataset_test, batch_size=1)                    # we are evaluating one graph at a time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can observe our dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Train samples (GRAPHS): ')\n",
    "print(f'Input: {dataset_train}')\n",
    "print(f' ')\n",
    "print(f'Validation samples (GRAPHS): ')\n",
    "print(f'Input: {dataset_valid}')\n",
    "print(f' ')\n",
    "print(f'Info inside one sample (ONE GRAPH): ')\n",
    "print(f'Input shape: {dataset_train[0].x.shape}')\n",
    "print(f'Node Features size: {dataset_train[0].num_node_features}')\n",
    "print(f'Target shape: {dataset_train[0].y.shape}')\n",
    "print(f'Edge Features shape: {dataset_train[0].edge_attr.shape}')\n",
    "print(f'Edge Features size: {dataset_train[0].num_edge_features}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are now ready to start to build our GNN model. Remember that we are exploiting the properties of the real deployment. Therefore, we can have features on the edges and on the nodes of the graph representation. Consequently, we can leverage from a GNN model that exploits such properties and computes the features for the edges and for the nodes in two separate models (i.e., 'EdgeModel' and 'NodeModel'). So far, the only GNN layer implemented in PyTorch Geometric that is able to do that, is the so called [MetaLayer](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/meta.html#MetaLayer). Thus, we built a GNN model using two MetaLayers having the same configuration. The input to those layers matches the input of the data. All layer use ReLU as activation function. The last layer has dimension 1, since we are trying to predict only one value (i.e., throughput). The architecture of the model was found following a trial and error strategy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Model's Architecture\n",
    "########################################################################################################################\n",
    "\n",
    "class EdgeModel(torch.nn.Module):\n",
    "    def __init__(self, n_node_features, n_edge_features, hiddens, n_targets):\n",
    "        super().__init__()\n",
    "        self.edge_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2 * n_node_features + n_edge_features, hiddens),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hiddens, n_targets),\n",
    "        )\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u=None, batch=None):\n",
    "        out = torch.cat([src, dest, edge_attr], 1)\n",
    "        out = self.edge_mlp(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class NodeModel(torch.nn.Module):\n",
    "    def __init__(self, n_node_features, hiddens, n_targets):\n",
    "        super(NodeModel, self).__init__()\n",
    "        self.node_mlp_1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hiddens, hiddens),\n",
    "        )\n",
    "        self.node_mlp_2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_node_features + hiddens, hiddens),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hiddens, n_targets),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([x[col], edge_attr], dim=1)\n",
    "        out = self.node_mlp_1(out)\n",
    "        out = scatter_mean(out, row, dim=0, dim_size=x.size(0))\n",
    "        out = torch.cat([x, out], dim=1)\n",
    "        out = self.node_mlp_2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MetaNet(torch.nn.Module):\n",
    "    def __init__(self, n_node_features, n_edge_features, num_hidden):\n",
    "        super(MetaNet, self).__init__()\n",
    "\n",
    "        # Input Layer\n",
    "        self.input = MetaLayer(\n",
    "            edge_model=EdgeModel(\n",
    "                n_node_features=n_node_features, n_edge_features=n_edge_features,\n",
    "                hiddens=num_hidden, n_targets=num_hidden),\n",
    "            node_model=NodeModel(n_node_features=n_node_features, hiddens=num_hidden, n_targets=num_hidden)\n",
    "            )\n",
    "\n",
    "        # Output Layer\n",
    "        self.output = MetaLayer(\n",
    "            edge_model=EdgeModel(\n",
    "                n_node_features=num_hidden, n_edge_features=num_hidden,\n",
    "                hiddens=num_hidden, n_targets=num_hidden),\n",
    "            node_model=NodeModel(n_node_features=num_hidden, hiddens=num_hidden, n_targets=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, y = data.x, data.edge_index, data.edge_attr, data.y\n",
    "\n",
    "        x, edge_attr, _ = self.input(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x, edge_attr, _ = self.output(x, edge_index, edge_attr)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we can set up the configuration we want. For example, we can select the number of hidden units for the MetaLayers, the number of training epochs, the log interval and the directory where we want to save our best model. As suggested by PyTorch, we select the computing device that better suit the computations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "## Network Configuration\n",
    "num_node_features = dataset_train[0].x.shape[1]\n",
    "num_edge_features = dataset_train[0].edge_attr.shape[1]\n",
    "num_hidden = 128\n",
    "\n",
    "\n",
    "## Training Configuration\n",
    "NUM_EPOCHS = 10\n",
    "LOG_INTERVAL = 1\n",
    "CHECKPOINT_INTERVAL = 1\n",
    "CHECKPOINT_DIR = \"checkpoints/gnn\"\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 5e-4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "Path(CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Compute device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "# Create model.\n",
    "model = MetaNet(num_node_features, num_edge_features, num_hidden).to(device)\n",
    "# Configure optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, PyTorch does not have an automatic method for training and evaluating models such as Tensorflow (i.e., 'model.fit'). Therefore, we need to implement the training loop and the evaluation loop."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#######################################################################################################################\n",
    "# Training utilities.\n",
    "########################################################################################################################\n",
    "\n",
    "# train step\n",
    "def train(dataset):\n",
    "    # Monitor training.\n",
    "    losses = []\n",
    "\n",
    "    # Put model in training mode!\n",
    "    model.train()\n",
    "    for batch in dataset:\n",
    "        # Training step.\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "        loss = torch.sqrt(F.mse_loss(out.squeeze()[batch.y_mask], batch.y[batch.y_mask]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Monitoring\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Return training metrics.\n",
    "    return losses\n",
    "\n",
    "# evaluation step\n",
    "def evaluate(dataset):\n",
    "    # Monitor evaluation.\n",
    "    losses = []\n",
    "    rmse = []\n",
    "\n",
    "    # Validation (1)\n",
    "    model.eval()\n",
    "    for batch in dataset:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Calculate validation losses.\n",
    "        out = model(batch)\n",
    "        loss = torch.sqrt(F.mse_loss(out.squeeze()[batch.y_mask], batch.y[batch.y_mask]))\n",
    "\n",
    "        rmse_batch = scores(batch, out)\n",
    "\n",
    "        # Metric logging.\n",
    "        losses.append(loss.item())\n",
    "        rmse.append(rmse_batch.item())\n",
    "\n",
    "    return losses, rmse\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    \"\"\"\n",
    "    Taken from https://towardsdatascience.com/how-to-save-and-load-a-model-in-pytorch-with-a-complete-example-c2920e617dee\n",
    "    state: checkpoint we want to save\n",
    "    is_best: is this the best checkpoint; min validation loss\n",
    "    checkpoint_path: path to save checkpoint\n",
    "    best_model_path: path to save best model\n",
    "    \"\"\"\n",
    "    f_path = checkpoint_path\n",
    "    # save checkpoint data to the path given, checkpoint_path\n",
    "    torch.save(state, f_path)\n",
    "    # if it is a best model, min validation loss\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        # copy that checkpoint file to best path given, best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to train our model. We save the model every checkpoint interval."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Training loop.\n",
    "########################################################################################################################\n",
    "\n",
    "best_model_path = os.path.join(CHECKPOINT_DIR, 'best_model')\n",
    "Path(best_model_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Metrics recorder per epoch.\n",
    "train_losses = []\n",
    "\n",
    "valid_losses = []\n",
    "valid_losses_corrected = []\n",
    "\n",
    "# Initialize tracker for minimum validation loss\n",
    "valid_loss_min = valid_loss_min_input = np.Inf\n",
    "\n",
    "# Training loop.\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train.\n",
    "    train_epoch_losses = train(train_loader)\n",
    "    valid_epoch_losses, valid_epoch_losses_corrected = evaluate(valid_loader)\n",
    "\n",
    "    # Log training metrics.\n",
    "    train_avg_loss = np.mean(train_epoch_losses)\n",
    "    train_losses.append(train_avg_loss)\n",
    "\n",
    "    # Log validation metrics.\n",
    "    valid_avg_loss = np.mean(valid_epoch_losses)\n",
    "    valid_losses.append(valid_avg_loss)\n",
    "\n",
    "    valid_avg_loss_corrected = np.mean(valid_epoch_losses_corrected)\n",
    "    valid_losses_corrected.append(valid_avg_loss_corrected)\n",
    "\n",
    "    # Print metrics\n",
    "    if epoch % LOG_INTERVAL == 0:\n",
    "        print(f\"epoch={epoch}, train_loss={train_avg_loss}, valid_loss={valid_avg_loss}, valid_loss*={valid_avg_loss_corrected}\")\n",
    "\n",
    "    if epoch % CHECKPOINT_INTERVAL == 0:\n",
    "        # compose checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': train_avg_loss,\n",
    "        }\n",
    "\n",
    "        checkpoint_fn = os.path.join(CHECKPOINT_DIR, f'checkpoint-{epoch}.tar')\n",
    "        save_ckp(checkpoint, False, checkpoint_fn, checkpoint_fn)\n",
    "\n",
    "\n",
    "    if valid_avg_loss_corrected <= valid_loss_min:\n",
    "        # save checkpoint as best model\n",
    "        best_model_fn = os.path.join(best_model_path, f'best-model.tar')\n",
    "        save_ckp(checkpoint, True, checkpoint_fn, best_model_fn)\n",
    "        valid_loss_min = valid_avg_loss_corrected\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now see how the performance during training was."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Training history.\n",
    "########################################################################################################################\n",
    "\n",
    "# summarize history for RMSE\n",
    "plt.plot(train_losses)\n",
    "plt.plot(valid_losses_corrected)\n",
    "plt.title('Model RMSE')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember that only ten epochs is too few to determine if the model is performing good or not. Thus, we need to train at least for 100 or even 1000 epochs to see if the model can lower the validation loss. Similarly to the other notebook, after the model is trained we can generate some predictions. As imagined, the throughput of the STAs was approximated by the model, but we did not take into account the throughput of the APs (the loss is only calculated for the STAs with ```batch.y_mask```). We calculate the AP's throughput by summing the throughput of its associated STAs.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Model.\n",
    "best_model_location = os.path.join(best_model_path, 'best-model.tar')\n",
    "checkpoint = torch.load(best_model_location, map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "# and where the prediction are going to be saved\n",
    "predictions_location = os.path.join(\"predictions\", \"gnn\")\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "# Monitor evaluation.\n",
    "losses = []\n",
    "rmse = []\n",
    "\n",
    "# Validation (1)\n",
    "model.eval()\n",
    "for batch in test_loader:\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    # Calculate validation losses over all devices.\n",
    "    with torch.no_grad():\n",
    "        out = model(batch)\n",
    "\n",
    "    preds_ap_sta = out.detach().numpy()\n",
    "\n",
    "    # Calculate validation losses over stations only\n",
    "    with torch.no_grad():\n",
    "        out = model(batch)\n",
    "\n",
    "    # Extract station predictions and corresponding access point labels.\n",
    "    station_predictions = out[batch.y_mask]\n",
    "    station_labels = batch.node_ap[batch.y_mask]\n",
    "\n",
    "    # Aggregate (sum) station predictions per access point and update model predictions.\n",
    "    ap_predictions = access_point_throughputs(station_predictions, station_labels).to(device)\n",
    "    out[~batch.y_mask] = ap_predictions\n",
    "\n",
    "    preds_sta = out.detach().numpy()\n",
    "\n",
    "    deployment = batch.deployment.detach().numpy()[0]\n",
    "    scenario = batch.scenario[0]\n",
    "\n",
    "    predict_loc = os.path.join(predictions_location, scenario)\n",
    "    os.makedirs(predict_loc, exist_ok=True)\n",
    "    predict_fn = 'throughput_{}.csv'.format(deployment)\n",
    "    predict_loc = os.path.join(predict_loc, predict_fn)\n",
    "    df = pd.DataFrame(data=preds_sta, columns=['thr_pred'])\n",
    "    df.to_csv(predict_loc, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the predictions, we can take the Root Mean Square Error to evaluate the accuracy of the model. We will do it in only one deployment, but you can easily replicate for all the deployment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select the scenario and the deployment to be evaluated.\n",
    "possible_scenarios = ['test_sce1', 'test_sce2', 'test_sce3', 'test_sce4']\n",
    "random_scenario = random.choice(possible_scenarios)\n",
    "random_deployment = random.choice(range(50))        # there are 50 deployments per test scenario.\n",
    "dpl_true = random_deployment + 1                    # the original files are from 1-50 while the predictions are from 0-49\n",
    "\n",
    "print(f\"Selected random scenario: {random_scenario}\")\n",
    "print(f\"Selected random deployment: {random_deployment}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the real values\n",
    "true_thr_location = os.path.join(\"datasets\", \"NDTDataset\", \"gnn\", \"raw\", \"test\", \"output_simulator\", random_scenario + \"_output\", \"throughput_\"+ str(dpl_true) + \".csv\")\n",
    "true_thr = pd.read_csv(true_thr_location, header=None)\n",
    "true_thr = true_thr.T[0].to_list()\n",
    "\n",
    "# load predicted values\n",
    "predicted_thr_location = os.path.join(\"predictions\", \"gnn\", random_scenario, \"throughput_\"+ str(random_deployment) + \".csv\")\n",
    "predicted_thr = pd.read_csv(predicted_thr_location)\n",
    "predicted_thr = predicted_thr[\"thr_pred\"].to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Root Mean Square Error (RMSE) is a risk function that helps us determine the average squared difference between the predicted and the actual value of a feature or variable. Usually, a RMSE score of less than 180 is considered a good score for a moderately or well working algorithm. In case, the RMSE value exceeds 180, we need to perform feature selection and hyper parameter tuning on the parameters of the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MSE = np.square(np.subtract(true_thr,predicted_thr)).mean()\n",
    "RMSE = math.sqrt(MSE)\n",
    "print(f\"Root Mean Square Error: {RMSE}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
