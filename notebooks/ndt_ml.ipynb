{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    " # Part 2: Building an NDT for Next-Generation WLANs with traditional AI/ML/DL"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This notebook will process the provided dataset to create a NDT using techniques such as Convolutional Neural Networks(CNNs) and Feed Forward Neural Networks (FNNs). The dataset was created for the 2020 edition of the [ITU AI for 5G Challenge](https://challenge.aiforgood.itu.int/) in the wireless throughput prediction statement. The dataset was created using [Komondor](https://github.com/wn-upf/Komondor), a simulator for next-generation, high-density WLANs which includes novel functionalities such as channel bonding and spatial reuse. The generated data includes simulated data from IEEE 802.11 WLAN deployments applying Dynamic Channel Bonding, using different random parameters, including channel allocation, location of nodes, and number of STAs per AP. The original dataset can be found in [here](https://doi.org/10.5281/zenodo.4106127). Then, the dataset was parsed using the file ```parse_original_dataset.py``` and then uploaded to Google Drive.  We include a fixed 80/20 split, where, from the original dataset (600 deployments in total), 80% of the deployments were used for training and while the remaining 20% was used for validation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start by importing the necessary libraries. To build the CNN and the FNN model, we are using Tensorflow and Keras. Additionally, we are using scikit-learn to build the Gradient Boost regressor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Standard python libraries.\n",
    "import os\n",
    "from pathlib import Path\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# General data science libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Deep learning libraries.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# now we can import all the functions from the auxiliaries' folder.\n",
    "from auxiliaries.dataset_ml import NDTDataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we proceed to download, extract, and accommodate the dataset in a proper format to be used by the different ML models we create later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Dataset.\n",
    "########################################################################################################################\n",
    "\n",
    "root = './datasets/NDTDataset/ml'\n",
    "\n",
    "# Create Datasets\n",
    "Data = NDTDataset(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset is comprised from two kind of files, one is the \"input_node_files.csv\" which contains the deployment information and their respective configuration (e.g., channels bonded, node location, WLAN code). This information is given to Komondor which simulates 100 seconds of transmission and the output from the simulator is given in the \"output_simulator\" files (e.g., airtime, obtained throughput, interference and measured SINR and RSSI). We use content from both files to compose the feature matrix. The features taken from the original dataset are the following.\n",
    "\n",
    "1. 'device_type': the considered device is an AP (0) or a STA (1).\n",
    "2. 'device_x_coordinate': the x coordinate of the device spatial position.\n",
    "3. 'device_y_coordinate': the y coordinate of the device spatial position.\n",
    "4. 'device_primary_channel': the primary channel of the AP.\n",
    "5. 'min_channel_allowed': the minimum channel allowed to perform channel bonding.\n",
    "6. 'max_channel_allowed': the maximum channel allowed to perform channel bonding.\n",
    "7. 'sinr': the obtained Signal plus Interference to Noise Ratio (SINR) between the AP and the STA. The feature is 0 for AP devices.\n",
    "8. 'mean_airtime': the mean percentage of airtime given by the used channel.\n",
    "9. 'mean_interference': the mean measured interferences between APs.\n",
    "10. 'rssi': the obtained Received Signal Strength Indicator (RSSI) between the AP and the STA. The feature is 0 for AP devices.\n",
    "11. 'distance_to_ap': the euclidean distance between AP and STA. The feature is 0 for AP devices.\n",
    "12. 'bandwidth': the total bandwidth used. We consider 20 MHz per used channel.\n",
    "\n",
    "We created a dataset that contains only the STAs, another just containing the APs and another containing all the devices. For this tutorial we will use the dataset which uses only the STAs for the computation. During its processing we encode the channel configuration ('device_primary_channel', 'min_channel_allowed', 'max_channel_allowed') using one hot encoder, which extends it into a vector of six positions. Moreover, if the model we will build is a CNN, then we have to perform a reshaping. Let's load the dataset in memory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the dataset you want to use.\n",
    "DATASET = \"sta\"\n",
    "MODEL = \"xgb\"\n",
    "\n",
    "# Load Train Dataset\n",
    "data_name = 'data_%s.npz' % DATASET\n",
    "data_path = os.path.join(Data.processed_dir, 'train', data_name)\n",
    "print(f'Loading {data_path}')\n",
    "with np.load(data_path) as data:\n",
    "    x_name = 'x_%s' % DATASET\n",
    "    y_name = 'y_%s' % DATASET\n",
    "    train_examples = data[x_name]\n",
    "    if MODEL == 'cnn':\n",
    "        # additionally, if you want to use the CNN model, then the samples must be reshaped.\n",
    "        train_examples = np.reshape(train_examples, (train_examples.shape[0], train_examples.shape[1], 1))\n",
    "    train_labels = data[y_name]\n",
    "\n",
    "# Load Validation Dataset\n",
    "data_name = 'data_%s.npz' % DATASET\n",
    "data_path = os.path.join(Data.processed_dir, 'valid', data_name)\n",
    "print(f'Loading {data_path}')\n",
    "with np.load(data_path) as data:\n",
    "    x_name = 'x_%s' % DATASET\n",
    "    y_name = 'y_%s' % DATASET\n",
    "    valid_examples = data[x_name]\n",
    "    if MODEL == 'cnn':\n",
    "        # additionally, if you want to use the CNN model, then the samples must be reshaped.\n",
    "        valid_examples = np.reshape(valid_examples, (valid_examples.shape[0], valid_examples.shape[1], 1))\n",
    "    valid_labels = data[y_name]\n",
    "\n",
    "# Load Test Dataset\n",
    "data_name = 'data_%s.npz' % DATASET\n",
    "data_path = os.path.join(Data.processed_dir, 'test', data_name)\n",
    "print(f'Loading {data_path}')\n",
    "with np.load(data_path) as data:\n",
    "    x_name = 'x_%s' % DATASET\n",
    "    y_name = 'y_%s' % DATASET\n",
    "    sce_name = 'sce_%s' % DATASET\n",
    "    dpl_name = 'dpl_%s' % DATASET\n",
    "    id_name = 'ids_%s' % DATASET\n",
    "    ap_name = 'aps_%s' % DATASET\n",
    "\n",
    "    test_examples = data[x_name]\n",
    "    if MODEL == 'cnn':\n",
    "        # additionally, if you want to use the CNN model, then the samples must be reshaped.\n",
    "        test_examples = np.reshape(test_examples, (test_examples.shape[0], test_examples.shape[1], 1))\n",
    "    test_labels = data[y_name]\n",
    "    test_scenarios = data[sce_name]             # array of the scenario where the test device belongs to, e.g., sce_1\n",
    "    test_deployments = data[dpl_name]           # array of the deployment where the test device belongs to, e.g., deployment 0\n",
    "    test_ids = data[id_name]                    # array with the id of all test devices\n",
    "    test_aps = data[ap_name]                    # array with the id of the APs in the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can observe our dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Train samples shapes: ')\n",
    "print(f'Input shape: {train_examples.shape}')\n",
    "print(f'Target shape: {train_labels.shape}')\n",
    "print(f' ')\n",
    "print(f'Validation samples shapes: ')\n",
    "print(f'Input shape: {valid_examples.shape}')\n",
    "print(f'Target shape: {valid_labels.shape}')\n",
    "print(f' ')\n",
    "print(f'Test samples shapes: ')\n",
    "print(f'Input shape: {test_examples.shape}')\n",
    "print(f'Target shape: {test_labels.shape}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are now ready to start to build our ML models. Remember that a typical ML model has a fixed input size, depending on the feature matrix. In our case, we propose three models, one Feed Forward Neural Network (FNN), one Convolutional Neural Network (CNN), and a Gradient Boost (xGB) Regressor. The FNN model has three dense (Fully Connected) layers, whose hidden units decreases in an exponential scale (e.g., 32, 16, 8). The CNN model has two 1D convolutional layers since we don't have an extra dimension (e.g., time) in our data in which we could operate. In general, the first layer (dense or convolutional), matches the input of the data (e.g., BATCH_SIZE X 15). The last layers of the CNN model are fully connected, therefore, the first two layers perform the feature extraction, and the last layers perform the prediction. All layer use ReLU as activation function. The last layer has dimension 1, since we are trying to predict only one value (i.e., throughput). The architecture of both models was found following a trial and error strategy. For the xGB, we used the default parameters given from scikit-learn."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Define model's architecture\n",
    "########################################################################################################################\n",
    "\n",
    "def build_model_fnn(num_input, num_hidden, dropout=0.05):\n",
    "    root = int(math.log2(num_hidden))\n",
    "    num_hidden_half = 2**(root - 1)\n",
    "    num_hidden_fourth = 2**(root - 2)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(num_hidden, kernel_initializer='normal', activation='relu', input_shape=(num_input,)),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(num_hidden_half, kernel_initializer='normal', activation='relu'),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "        tf.keras.layers.Dense(num_hidden_fourth, kernel_initializer='normal', activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='relu')\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_cnn(num_input, num_hidden, dropout=0.05, kernel_size=3, num_hidden_dense=8, num_hidden_dense_2=8):\n",
    "    model = tf.keras.Sequential()\n",
    "    # conv 1\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv1D(\n",
    "            num_hidden,\n",
    "            kernel_size=kernel_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "            input_shape=(num_input, 1)\n",
    "        )\n",
    "    )\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "    # conv 2\n",
    "    model.add(\n",
    "        tf.keras.layers.Conv1D(\n",
    "            num_hidden,\n",
    "            kernel_size=kernel_size,\n",
    "            padding='same',\n",
    "            activation='relu'\n",
    "        )\n",
    "    )\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "    # Dense 1\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(num_hidden_dense, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "    # Dense 2\n",
    "    model.add(tf.keras.layers.Dense(num_hidden_dense_2, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(dropout))\n",
    "    # Dense 3\n",
    "    model.add(tf.keras.layers.Dense(1, activation='relu'))\n",
    "    return model\n",
    "\n",
    "def build_gb_model():\n",
    "    reg = GradientBoostingRegressor()\n",
    "    return reg"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we can set up the configuration we want. For example, we can select the number of initial hidden units for the intermediate layers, the number of training epochs, the log interval and the directory where we want to save our best model. The selected optimizer is adam, which gave the best performance during an exploratory analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "## Network Configuration\n",
    "num_input = train_examples.shape[1]\n",
    "NUM_HIDDEN = 16\n",
    "# DROPOUT = 0.05\n",
    "# KERNEL_SIZE = 3\n",
    "# NUM_HIDDEN_DENSE = 8\n",
    "# NUM_HIDDEN_DENSE_2 = 8\n",
    "\n",
    "## Training Configuration\n",
    "NUM_EPOCHS = 10\n",
    "LOG_INTERVAL = 1\n",
    "CHECKPOINT_INTERVAL = 1\n",
    "CHECKPOINT_DIR = \"checkpoints/ml\"\n",
    "LEARNING_RATE = 0.01\n",
    "WEIGHT_DECAY = 5e-4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create model.\n",
    "if MODEL == 'fnn':\n",
    "    model = build_model_fnn(num_input, NUM_HIDDEN)\n",
    "elif MODEL == 'cnn':\n",
    "    model = build_model_cnn(num_input, NUM_HIDDEN)\n",
    "else:\n",
    "    model = build_gb_model()                #\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are ready to train our model. For saving the best model, we create a ModelCheckpoint callback, which is activated, once the validation loss is lower that the previous saved model. For the xGB, the model is saved every epoch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path = os.path.join(CHECKPOINT_DIR, MODEL)\n",
    "best_model_path = os.path.join(model_path, 'best_model')\n",
    "Path(best_model_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if MODEL != \"xgb\":\n",
    "    ########################################################################################################################\n",
    "    # Training loop for FNN and CNN models\n",
    "    ########################################################################################################################\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    model.summary()\n",
    "\n",
    "    # Define Callbacks\n",
    "    model_name = f\"model_{DATASET}_{MODEL}.h5\"\n",
    "    print(f\"best model will be saved under: {model_name}\")\n",
    "    path_checkpoint = os.path.join(best_model_path, model_name)\n",
    "    # we only save the best model: if the monitored variable is lower than previously saved\n",
    "    modelckpt_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        filepath=path_checkpoint,\n",
    "        verbose=0,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "    )\n",
    "\n",
    "    # Train model on dataset\n",
    "    history = model.fit(\n",
    "        x=train_examples,\n",
    "        y=train_labels,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        verbose=2,\n",
    "        validation_data=(valid_examples, valid_labels),\n",
    "        callbacks=[modelckpt_callback]\n",
    "    )\n",
    "else:\n",
    "    xgb_evaluation = []\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"processing epoch : {epoch}\")\n",
    "        model.fit(train_examples, train_labels)\n",
    "        y_pred = model.predict(valid_examples)\n",
    "        rmse = mean_squared_error(valid_labels, y_pred, squared=False)\n",
    "        xgb_evaluation.append(rmse)\n",
    "        # save model to file\n",
    "        model_name = f\"model_{DATASET}_{MODEL}_{epoch}.dat\"\n",
    "        model_filename = os.path.join(best_model_path, model_name)\n",
    "        pickle.dump(model, open(model_filename, \"wb\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now see how the performance during training was."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if MODEL != \"xgb\":\n",
    "    ########################################################################################################################\n",
    "    # Training history for FNN and CNN models\n",
    "    ########################################################################################################################\n",
    "\n",
    "    # summarize history for RMSE\n",
    "    plt.plot(history.history['root_mean_squared_error'])\n",
    "    plt.plot(history.history['val_root_mean_squared_error'])\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "else:\n",
    "    plt.plot(xgb_evaluation)\n",
    "    plt.legend(['train'], loc='upper left')\n",
    "\n",
    "plt.title('Model RMSE')\n",
    "plt.ylabel('RMSE')\n",
    "plt.xlabel('epoch')\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember that only ten epochs is too few to determine if the model is performing good or not. Thus, we need to train at least for 100 or even 1000 epochs to see if the model can lower the validation loss.\n",
    "\n",
    "Now we can evaluate the model in the test dataset. For doing so, we need first to make predictions of the final throughput per device. Notice that, since we used the dataset that only has the STAs and not the APs, the AP's throughput must be actually calculated in an additional step. Moreover, if using the dataset that has all the devices (a.k.a. 'all'), might be that the throughput of the AP is not the sum of the throughput of its associated devices. The dataset that has only APs does not need to be post-processed and we cannot retrieve the throughput of the STAs."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# let's select where the weights of the best model were saved.\n",
    "model_location = os.path.join(best_model_path, model_name)\n",
    "# and where the prediction are going to be saved\n",
    "predictions_location = os.path.join(\"predictions\", model_name)\n",
    "\n",
    "if MODEL != \"xgb\":\n",
    "    # Load Model\n",
    "    model.load_weights(model_location)\n",
    "else:\n",
    "    model = pickle.load(open(model_location, \"rb\"))\n",
    "\n",
    "# Make Predictions --> Not aggregated throughput\n",
    "sta_pred = model.predict(test_examples)\n",
    "if MODEL == \"xgb\":\n",
    "    #Reshape to add extra dimension\n",
    "    sta_pred = sta_pred.reshape(-1,1)\n",
    "\n",
    "\n",
    "# Add predictions to aggregated throughput of AP\n",
    "total_devices = test_ids[-1] - test_ids[0] + 2\n",
    "# new array that is going to save the throughput of all devices. Throughput of AP is aggregated by its associated STAs\n",
    "thr_devices = np.zeros(total_devices)\n",
    "scenarios = np.zeros(total_devices, dtype='<U9')\n",
    "deployments = np.zeros(total_devices)\n",
    "for associated_ap in np.unique(test_aps):\n",
    "    thr_ap = 0\n",
    "    ix_ap = associated_ap - test_aps[0]  # index AP in new array\n",
    "    j = 1\n",
    "    for i in range(len(test_aps)):\n",
    "        if test_aps[i] == associated_ap:\n",
    "            thr_ap += sta_pred[i][0]  # aggregated throughput AP\n",
    "            thr_devices[ix_ap+j] = sta_pred[i][0] # throughput STA\n",
    "            sce = test_scenarios[i]\n",
    "            dpl = test_deployments[i]\n",
    "            scenarios[ix_ap+j] = sce\n",
    "            deployments[ix_ap+j]= dpl\n",
    "            j += 1\n",
    "    thr_devices[ix_ap] = thr_ap\n",
    "    scenarios[ix_ap] = sce\n",
    "    deployments[ix_ap] = dpl\n",
    "\n",
    "# Visualize Results\n",
    "thr_devices = thr_devices.reshape(-1, 1)\n",
    "scenarios = scenarios.reshape(-1, 1)\n",
    "deployments = deployments.reshape(-1, 1)\n",
    "numpy_data = np.hstack([thr_devices, scenarios, deployments])\n",
    "df = pd.DataFrame(data=numpy_data, columns=['thr_pred', 'scenario', 'deployment'])\n",
    "\n",
    "# Divide and save predictions to CSV\n",
    "grouped_df = df.groupby(['scenario', 'deployment'])\n",
    "for key, item in grouped_df:\n",
    "    sce, dpl = key[0], int(float(key[1]))\n",
    "    group = grouped_df.get_group(key)\n",
    "    predict_loc = os.path.join(predictions_location, sce)\n",
    "    os.makedirs(predict_loc, exist_ok=True)\n",
    "    predict_fn = 'throughput_{}.csv'.format(dpl)\n",
    "    predict_loc = os.path.join(predict_loc, predict_fn)\n",
    "    group.to_csv(predict_loc, columns=['thr_pred'], index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have the predictions, we can take the Root Mean Square Error to evaluate the accuracy of the model. We will do it in only one deployment, but you can easily replicate for all the deployment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select the scenario and the deployment to be evaluated.\n",
    "possible_scenarios = ['test_sce1', 'test_sce2', 'test_sce3', 'test_sce4']\n",
    "random_scenario = random.choice(possible_scenarios)\n",
    "random_deployment = random.choice(range(50))        # there are 50 deployments per test scenario.\n",
    "dpl_true = random_deployment + 1                    # the original files are from 1-50 while the predictions are from 0-49\n",
    "\n",
    "print(f\"Selected random scenario: {random_scenario}\")\n",
    "print(f\"Selected random deployment: {random_deployment}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we can proceed to load the the real values and compare them to the predicted values for the selected scenario."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the real values\n",
    "true_thr_location = os.path.join(\"datasets\", \"NDTDataset\", \"ml\", \"raw\", \"test\", \"output_simulator\", random_scenario + \"_output\", \"throughput_\"+ str(dpl_true) + \".csv\")\n",
    "true_thr = pd.read_csv(true_thr_location, header=None)\n",
    "true_thr = true_thr.T[0].to_list()\n",
    "\n",
    "# load predicted values\n",
    "predicted_thr_location = os.path.join(\"predictions\", model_name, random_scenario, \"throughput_\"+ str(random_deployment) + \".csv\")\n",
    "predicted_thr = pd.read_csv(predicted_thr_location)\n",
    "predicted_thr = predicted_thr[\"thr_pred\"].to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Root Mean Square Error (RMSE) is a risk function that helps us determine the average squared difference between the predicted and the actual value of a feature or variable. Usually, a RMSE score of less than 180 is considered a good score for a moderately or well working algorithm. In case, the RMSE value exceeds 180, we need to perform feature selection and hyper parameter tuning on the parameters of the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MSE = np.square(np.subtract(true_thr,predicted_thr)).mean()\n",
    "RMSE = math.sqrt(MSE)\n",
    "print(f\"Root Mean Square Error: {RMSE}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('gnn-tutorial')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2268c3739cfabf18051b3f7c40d7e384210b4da404395d10f1415f3c399fda42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
